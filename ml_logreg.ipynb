{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ1TfHmpAk4wtbAZjmhZ1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6239a53c211d4a27a29c1ef9214901cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ae49daf3cbc475d8176b2bb1687cc4a",
              "IPY_MODEL_358bdf1e4313404fb3c9a4d4b52bcfe6",
              "IPY_MODEL_ad82f25131ed406faf7f2b53df9bd76d"
            ],
            "layout": "IPY_MODEL_a120bad639ea4abb8f536b4b0a700efb"
          }
        },
        "5ae49daf3cbc475d8176b2bb1687cc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c3fe1daef1941238450a7a61422bd97",
            "placeholder": "​",
            "style": "IPY_MODEL_3e97ae4766e54c9e87d0311d02865fac",
            "value": "100%"
          }
        },
        "358bdf1e4313404fb3c9a4d4b52bcfe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da69feb4b1eb4c9a8406191bde8e386c",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfcb5e45709a48458b870d7282d5cbba",
            "value": 15
          }
        },
        "ad82f25131ed406faf7f2b53df9bd76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5d0e01d1efb4d92ad60b6c1f80c52e7",
            "placeholder": "​",
            "style": "IPY_MODEL_87eec307f80d4afd94548ae64e47ee8b",
            "value": " 15/15 [00:00&lt;00:00, 36.34it/s]"
          }
        },
        "a120bad639ea4abb8f536b4b0a700efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3fe1daef1941238450a7a61422bd97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e97ae4766e54c9e87d0311d02865fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da69feb4b1eb4c9a8406191bde8e386c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfcb5e45709a48458b870d7282d5cbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5d0e01d1efb4d92ad60b6c1f80c52e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87eec307f80d4afd94548ae64e47ee8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saravanan-039/ML-challenge/blob/models/ml_logreg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# LogisticRegression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer for text feature extraction\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')  # Replace 'your_data.csv' with the actual file name\n",
        "\n",
        "# Separate features and target variable\n",
        "# Exclude the 'ID' column from features\n",
        "X = data.drop(['Type', 'ID'], axis=1)  # Replace 'target_variable' with the name of your target column\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article']) # Transform the text data into numerical features\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "\n",
        "pred_df = pd.read_csv('unlabelled_test2.csv')  # Load into a DataFrame to preserve 'TestID'\n",
        "pred_text = pred_df['Article']  # Extract the 'Article' column for vectorization\n",
        "\n",
        "# Vectorize the 'Article' column\n",
        "pred_vectors = vectorizer.transform(pred_text)  # Transform the text data into numerical features\n",
        "y_pred = model.predict(pred_vectors)  # Predict using the vectorized data\n",
        "print(\"ID   Predicted Type\")\n",
        "# Iterate using the DataFrame for 'ID'\n",
        "# Use 'ID' instead of 'TestID'\n",
        "for test_id, prediction in zip(pred_df['ID'], y_pred):\n",
        "    predicted_label = 0 if prediction == \"Human-written\" else 1\n",
        "\n",
        "    print(test_id, predicted_label)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4nIEo6R64P",
        "outputId": "4c6d1ee0-2c95-4ae8-8b4b-01551693ff0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.77\n",
            "ID   Predicted Type\n",
            "TEST_1 0\n",
            "TEST_2 0\n",
            "TEST_3 0\n",
            "TEST_4 0\n",
            "TEST_5 0\n",
            "TEST_6 0\n",
            "TEST_7 0\n",
            "TEST_8 0\n",
            "TEST_9 0\n",
            "TEST_10 0\n",
            "TEST_11 0\n",
            "TEST_12 0\n",
            "TEST_13 0\n",
            "TEST_14 0\n",
            "TEST_15 0\n",
            "TEST_16 0\n",
            "TEST_17 0\n",
            "TEST_18 0\n",
            "TEST_19 0\n",
            "TEST_20 0\n",
            "TEST_21 0\n",
            "TEST_22 0\n",
            "TEST_23 0\n",
            "TEST_24 0\n",
            "TEST_25 0\n",
            "TEST_26 0\n",
            "TEST_27 0\n",
            "TEST_28 0\n",
            "TEST_29 0\n",
            "TEST_30 0\n",
            "TEST_31 0\n",
            "TEST_32 0\n",
            "TEST_33 0\n",
            "TEST_34 0\n",
            "TEST_35 0\n",
            "TEST_36 0\n",
            "TEST_37 0\n",
            "TEST_38 0\n",
            "TEST_39 0\n",
            "TEST_40 0\n",
            "TEST_41 0\n",
            "TEST_42 0\n",
            "TEST_43 0\n",
            "TEST_44 0\n",
            "TEST_45 0\n",
            "TEST_46 0\n",
            "TEST_47 0\n",
            "TEST_48 0\n",
            "TEST_49 0\n",
            "TEST_50 0\n",
            "TEST_51 0\n",
            "TEST_52 0\n",
            "TEST_53 0\n",
            "TEST_54 0\n",
            "TEST_55 0\n",
            "TEST_56 0\n",
            "TEST_57 0\n",
            "TEST_58 0\n",
            "TEST_59 0\n",
            "TEST_60 0\n",
            "TEST_61 0\n",
            "TEST_62 0\n",
            "TEST_63 0\n",
            "TEST_64 0\n",
            "TEST_65 0\n",
            "TEST_66 0\n",
            "TEST_67 1\n",
            "TEST_68 0\n",
            "TEST_69 0\n",
            "TEST_70 0\n",
            "TEST_71 0\n",
            "TEST_72 0\n",
            "TEST_73 0\n",
            "TEST_74 0\n",
            "TEST_75 0\n",
            "TEST_76 0\n",
            "TEST_77 0\n",
            "TEST_78 0\n",
            "TEST_79 0\n",
            "TEST_80 0\n",
            "TEST_81 0\n",
            "TEST_82 0\n",
            "TEST_83 0\n",
            "TEST_84 0\n",
            "TEST_85 0\n",
            "TEST_86 0\n",
            "TEST_87 0\n",
            "TEST_88 0\n",
            "TEST_89 0\n",
            "TEST_90 0\n",
            "TEST_91 0\n",
            "TEST_92 0\n",
            "TEST_93 0\n",
            "TEST_94 0\n",
            "TEST_95 0\n",
            "TEST_96 0\n",
            "TEST_97 0\n",
            "TEST_98 0\n",
            "TEST_99 0\n",
            "TEST_100 0\n",
            "TEST_101 0\n",
            "TEST_102 0\n",
            "TEST_103 0\n",
            "TEST_104 0\n",
            "TEST_105 0\n",
            "TEST_106 0\n",
            "TEST_107 0\n",
            "TEST_108 0\n",
            "TEST_109 0\n",
            "TEST_110 0\n",
            "TEST_111 0\n",
            "TEST_112 0\n",
            "TEST_113 0\n",
            "TEST_114 0\n",
            "TEST_115 0\n",
            "TEST_116 0\n",
            "TEST_117 0\n",
            "TEST_118 0\n",
            "TEST_119 0\n",
            "TEST_120 0\n",
            "TEST_121 0\n",
            "TEST_122 0\n",
            "TEST_123 0\n",
            "TEST_124 0\n",
            "TEST_125 0\n",
            "TEST_126 0\n",
            "TEST_127 0\n",
            "TEST_128 0\n",
            "TEST_129 0\n",
            "TEST_130 0\n",
            "TEST_131 0\n",
            "TEST_132 0\n",
            "TEST_133 0\n",
            "TEST_134 0\n",
            "TEST_135 0\n",
            "TEST_136 0\n",
            "TEST_137 0\n",
            "TEST_138 0\n",
            "TEST_139 0\n",
            "TEST_140 0\n",
            "TEST_141 1\n",
            "TEST_142 0\n",
            "TEST_143 0\n",
            "TEST_144 0\n",
            "TEST_145 0\n",
            "TEST_146 0\n",
            "TEST_147 0\n",
            "TEST_148 0\n",
            "TEST_149 0\n",
            "TEST_150 0\n",
            "TEST_151 0\n",
            "TEST_152 0\n",
            "TEST_153 0\n",
            "TEST_154 0\n",
            "TEST_155 0\n",
            "TEST_156 0\n",
            "TEST_157 0\n",
            "TEST_158 0\n",
            "TEST_159 0\n",
            "TEST_160 0\n",
            "TEST_161 0\n",
            "TEST_162 0\n",
            "TEST_163 0\n",
            "TEST_164 0\n",
            "TEST_165 0\n",
            "TEST_166 0\n",
            "TEST_167 0\n",
            "TEST_168 0\n",
            "TEST_169 0\n",
            "TEST_170 0\n",
            "TEST_171 0\n",
            "TEST_172 0\n",
            "TEST_173 0\n",
            "TEST_174 0\n",
            "TEST_175 0\n",
            "TEST_176 0\n",
            "TEST_177 0\n",
            "TEST_178 0\n",
            "TEST_179 0\n",
            "TEST_180 0\n",
            "TEST_181 0\n",
            "TEST_182 0\n",
            "TEST_183 0\n",
            "TEST_184 0\n",
            "TEST_185 0\n",
            "TEST_186 0\n",
            "TEST_187 0\n",
            "TEST_188 0\n",
            "TEST_189 0\n",
            "TEST_190 0\n",
            "TEST_191 0\n",
            "TEST_192 0\n",
            "TEST_193 0\n",
            "TEST_194 0\n",
            "TEST_195 0\n",
            "TEST_196 0\n",
            "TEST_197 0\n",
            "TEST_198 0\n",
            "TEST_199 0\n",
            "TEST_200 0\n",
            "TEST_201 0\n",
            "TEST_202 0\n",
            "TEST_203 0\n",
            "TEST_204 0\n",
            "TEST_205 0\n",
            "TEST_206 0\n",
            "TEST_207 0\n",
            "TEST_208 0\n",
            "TEST_209 0\n",
            "TEST_210 0\n",
            "TEST_211 0\n",
            "TEST_212 0\n",
            "TEST_213 0\n",
            "TEST_214 0\n",
            "TEST_215 0\n",
            "TEST_216 0\n",
            "TEST_217 0\n",
            "TEST_218 0\n",
            "TEST_219 0\n",
            "TEST_220 0\n",
            "TEST_221 1\n",
            "TEST_222 0\n",
            "TEST_223 0\n",
            "TEST_224 0\n",
            "TEST_225 0\n",
            "TEST_226 0\n",
            "TEST_227 0\n",
            "TEST_228 0\n",
            "TEST_229 0\n",
            "TEST_230 0\n",
            "TEST_231 0\n",
            "TEST_232 0\n",
            "TEST_233 0\n",
            "TEST_234 0\n",
            "TEST_235 0\n",
            "TEST_236 0\n",
            "TEST_237 0\n",
            "TEST_238 0\n",
            "TEST_239 0\n",
            "TEST_240 0\n",
            "TEST_241 0\n",
            "TEST_242 0\n",
            "TEST_243 0\n",
            "TEST_244 0\n",
            "TEST_245 0\n",
            "TEST_246 0\n",
            "TEST_247 0\n",
            "TEST_248 0\n",
            "TEST_249 0\n",
            "TEST_250 0\n",
            "TEST_251 0\n",
            "TEST_252 0\n",
            "TEST_253 0\n",
            "TEST_254 0\n",
            "TEST_255 0\n",
            "TEST_256 0\n",
            "TEST_257 0\n",
            "TEST_258 0\n",
            "TEST_259 0\n",
            "TEST_260 0\n",
            "TEST_261 0\n",
            "TEST_262 0\n",
            "TEST_263 0\n",
            "TEST_264 0\n",
            "TEST_265 0\n",
            "TEST_266 0\n",
            "TEST_267 0\n",
            "TEST_268 0\n",
            "TEST_269 0\n",
            "TEST_270 0\n",
            "TEST_271 0\n",
            "TEST_272 0\n",
            "TEST_273 0\n",
            "TEST_274 0\n",
            "TEST_275 0\n",
            "TEST_276 0\n",
            "TEST_277 0\n",
            "TEST_278 0\n",
            "TEST_279 0\n",
            "TEST_280 0\n",
            "TEST_281 0\n",
            "TEST_282 0\n",
            "TEST_283 0\n",
            "TEST_284 0\n",
            "TEST_285 0\n",
            "TEST_286 1\n",
            "TEST_287 0\n",
            "TEST_288 0\n",
            "TEST_289 0\n",
            "TEST_290 0\n",
            "TEST_291 0\n",
            "TEST_292 0\n",
            "TEST_293 0\n",
            "TEST_294 0\n",
            "TEST_295 0\n",
            "TEST_296 0\n",
            "TEST_297 0\n",
            "TEST_298 0\n",
            "TEST_299 0\n",
            "TEST_300 1\n",
            "TEST_301 0\n",
            "TEST_302 0\n",
            "TEST_303 0\n",
            "TEST_304 0\n",
            "TEST_305 0\n",
            "TEST_306 0\n",
            "TEST_307 0\n",
            "TEST_308 0\n",
            "TEST_309 0\n",
            "TEST_310 0\n",
            "TEST_311 0\n",
            "TEST_312 0\n",
            "TEST_313 0\n",
            "TEST_314 0\n",
            "TEST_315 0\n",
            "TEST_316 0\n",
            "TEST_317 0\n",
            "TEST_318 0\n",
            "TEST_319 0\n",
            "TEST_320 0\n",
            "TEST_321 0\n",
            "TEST_322 0\n",
            "TEST_323 0\n",
            "TEST_324 0\n",
            "TEST_325 0\n",
            "TEST_326 0\n",
            "TEST_327 0\n",
            "TEST_328 0\n",
            "TEST_329 0\n",
            "TEST_330 0\n",
            "TEST_331 0\n",
            "TEST_332 0\n",
            "TEST_333 0\n",
            "TEST_334 0\n",
            "TEST_335 0\n",
            "TEST_336 0\n",
            "TEST_337 0\n",
            "TEST_338 0\n",
            "TEST_339 0\n",
            "TEST_340 0\n",
            "TEST_341 1\n",
            "TEST_342 0\n",
            "TEST_343 0\n",
            "TEST_344 0\n",
            "TEST_345 0\n",
            "TEST_346 0\n",
            "TEST_347 0\n",
            "TEST_348 0\n",
            "TEST_349 0\n",
            "TEST_350 0\n",
            "TEST_351 0\n",
            "TEST_352 0\n",
            "TEST_353 0\n",
            "TEST_354 0\n",
            "TEST_355 0\n",
            "TEST_356 0\n",
            "TEST_357 0\n",
            "TEST_358 0\n",
            "TEST_359 0\n",
            "TEST_360 0\n",
            "TEST_361 0\n",
            "TEST_362 0\n",
            "TEST_363 0\n",
            "TEST_364 0\n",
            "TEST_365 0\n",
            "TEST_366 0\n",
            "TEST_367 0\n",
            "TEST_368 0\n",
            "TEST_369 0\n",
            "TEST_370 0\n",
            "TEST_371 0\n",
            "TEST_372 0\n",
            "TEST_373 0\n",
            "TEST_374 0\n",
            "TEST_375 0\n",
            "TEST_376 0\n",
            "TEST_377 0\n",
            "TEST_378 0\n",
            "TEST_379 0\n",
            "TEST_380 0\n",
            "TEST_381 0\n",
            "TEST_382 0\n",
            "TEST_383 0\n",
            "TEST_384 0\n",
            "TEST_385 0\n",
            "TEST_386 0\n",
            "TEST_387 0\n",
            "TEST_388 0\n",
            "TEST_389 0\n",
            "TEST_390 0\n",
            "TEST_391 0\n",
            "TEST_392 0\n",
            "TEST_393 0\n",
            "TEST_394 0\n",
            "TEST_395 0\n",
            "TEST_396 0\n",
            "TEST_397 0\n",
            "TEST_398 0\n",
            "TEST_399 0\n",
            "TEST_400 0\n",
            "TEST_401 0\n",
            "TEST_402 0\n",
            "TEST_403 0\n",
            "TEST_404 0\n",
            "TEST_405 0\n",
            "TEST_406 0\n",
            "TEST_407 0\n",
            "TEST_408 0\n",
            "TEST_409 0\n",
            "TEST_410 0\n",
            "TEST_411 0\n",
            "TEST_412 0\n",
            "TEST_413 0\n",
            "TEST_414 0\n",
            "TEST_415 0\n",
            "TEST_416 0\n",
            "TEST_417 0\n",
            "TEST_418 0\n",
            "TEST_419 0\n",
            "TEST_420 0\n",
            "TEST_421 0\n",
            "TEST_422 0\n",
            "TEST_423 0\n",
            "TEST_424 0\n",
            "TEST_425 0\n",
            "TEST_426 0\n",
            "TEST_427 0\n",
            "TEST_428 0\n",
            "TEST_429 0\n",
            "TEST_430 0\n",
            "TEST_431 0\n",
            "TEST_432 0\n",
            "TEST_433 0\n",
            "TEST_434 0\n",
            "TEST_435 0\n",
            "TEST_436 0\n",
            "TEST_437 0\n",
            "TEST_438 0\n",
            "TEST_439 0\n",
            "TEST_440 0\n",
            "TEST_441 0\n",
            "TEST_442 0\n",
            "TEST_443 0\n",
            "TEST_444 0\n",
            "TEST_445 0\n",
            "TEST_446 0\n",
            "TEST_447 0\n",
            "TEST_448 0\n",
            "TEST_449 0\n",
            "TEST_450 0\n",
            "TEST_451 0\n",
            "TEST_452 0\n",
            "TEST_453 0\n",
            "TEST_454 0\n",
            "TEST_455 0\n",
            "TEST_456 0\n",
            "TEST_457 0\n",
            "TEST_458 0\n",
            "TEST_459 0\n",
            "TEST_460 0\n",
            "TEST_461 0\n",
            "TEST_462 0\n",
            "TEST_463 0\n",
            "TEST_464 0\n",
            "TEST_465 0\n",
            "TEST_466 0\n",
            "TEST_467 0\n",
            "TEST_468 0\n",
            "TEST_469 0\n",
            "TEST_470 0\n",
            "TEST_471 0\n",
            "TEST_472 0\n",
            "TEST_473 0\n",
            "TEST_474 0\n",
            "TEST_475 0\n",
            "TEST_476 0\n",
            "TEST_477 0\n",
            "TEST_478 0\n",
            "TEST_479 0\n",
            "TEST_480 0\n",
            "TEST_481 0\n",
            "TEST_482 0\n",
            "TEST_483 0\n",
            "TEST_484 0\n",
            "TEST_485 0\n",
            "TEST_486 0\n",
            "TEST_487 0\n",
            "TEST_488 0\n",
            "TEST_489 0\n",
            "TEST_490 0\n",
            "TEST_491 0\n",
            "TEST_492 0\n",
            "TEST_493 0\n",
            "TEST_494 1\n",
            "TEST_495 0\n",
            "TEST_496 0\n",
            "TEST_497 0\n",
            "TEST_498 0\n",
            "TEST_499 0\n",
            "TEST_500 0\n",
            "TEST_501 0\n",
            "TEST_502 0\n",
            "TEST_503 0\n",
            "TEST_504 0\n",
            "TEST_505 0\n",
            "TEST_506 0\n",
            "TEST_507 0\n",
            "TEST_508 0\n",
            "TEST_509 0\n",
            "TEST_510 0\n",
            "TEST_511 0\n",
            "TEST_512 0\n",
            "TEST_513 0\n",
            "TEST_514 0\n",
            "TEST_515 0\n",
            "TEST_516 0\n",
            "TEST_517 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Support Vector Machines (SVM)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model\n",
        "model = SVC()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un4uhky7bTfL",
        "outputId": "5ec4d01b-284a-4d96-d15d-f1b34364a2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decision trees\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYJosksLt3--",
        "outputId": "e8b377fe-42c0-4ebc-8117-84917656a6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#random forests\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXRNazIfuGr5",
        "outputId": "136daddf-264e-461b-d3cd-3184c7ab85f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# naive bayes\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZcV42kNuV8x",
        "outputId": "f98cccbb-3718-4273-c949-cbf88652b906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient boosting\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting model\n",
        "model = GradientBoostingClassifier()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Kax_SgufBY",
        "outputId": "b1e885ee-1efd-4ee4-fa72-23d24f688712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k-nearest neighbors\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a k-Nearest Neighbors model\n",
        "model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (n_neighbors)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiTRYoL6vLse",
        "outputId": "ec4b6d2f-197e-44ac-ff53-3509554f6cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# neural networks\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Encode labels (assuming binary classification)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVNGGhKevVM4",
        "outputId": "4fe98da4-561d-457a-9c8c-5a863db77bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.6532 - loss: 0.6837 - val_accuracy: 0.7700 - val_loss: 0.6447\n",
            "Epoch 2/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7877 - loss: 0.6130 - val_accuracy: 0.7700 - val_loss: 0.5728\n",
            "Epoch 3/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7747 - loss: 0.5153 - val_accuracy: 0.7700 - val_loss: 0.5096\n",
            "Epoch 4/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7559 - loss: 0.4313 - val_accuracy: 0.7700 - val_loss: 0.4597\n",
            "Epoch 5/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7749 - loss: 0.3415 - val_accuracy: 0.7900 - val_loss: 0.3982\n",
            "Epoch 6/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9576 - loss: 0.2190 - val_accuracy: 0.8600 - val_loss: 0.3418\n",
            "Epoch 7/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1258 - val_accuracy: 0.8900 - val_loss: 0.3001\n",
            "Epoch 8/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0686 - val_accuracy: 0.9000 - val_loss: 0.2833\n",
            "Epoch 9/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0347 - val_accuracy: 0.9200 - val_loss: 0.2698\n",
            "Epoch 10/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0194 - val_accuracy: 0.9200 - val_loss: 0.2645\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9253 - loss: 0.2473 \n",
            "Accuracy: 0.9200000166893005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adaboost\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost model\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)  # You can adjust the base estimator\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)  # You can adjust the number of estimators\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzy4JaDux2a5",
        "outputId": "5968b417-63f6-4ac9-8636-aac2c8bc0e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Encode labels (assuming binary classification)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an XGBoost model\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wTiw5LiyIUI",
        "outputId": "6c091239-c4e9-41a8-bd93-c54a741e81ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# LightGBM\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Encode labels (assuming binary classification)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a LightGBM dataset\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "# Set parameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',  # Or 'auc' if you prefer AUC as the metric\n",
        "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
        "    'num_leaves': 31,  # Adjust as needed\n",
        "    'learning_rate': 0.05,  # Adjust as needed\n",
        "    'feature_fraction': 0.9,  # Adjust as needed\n",
        "    'bagging_fraction': 0.8,  # Adjust as needed\n",
        "    'bagging_freq': 5,  # Adjust as needed\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Train the LightGBM model\n",
        "num_round = 500  # Adjust the number of boosting rounds as needed\n",
        "# Pass early stopping as a parameter to a callback function\n",
        "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data], callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
        "y_pred_binary = [round(pred) for pred in y_pred]  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print('Accuracy:', accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlbc5-Mw055w",
        "outputId": "d35ace10-f884-4214-d7a2-876b40310040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid_0's binary_logloss: 0.352793\n",
            "Accuracy: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ensemble methods(stackin, blending)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('labelled_train_set.csv')  # Replace with your actual file name\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Define meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Create stacking classifier\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "# Train the stacking model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_stacking = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate the stacking model\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print('Stacking Accuracy:', accuracy_stacking)\n",
        "\n",
        "# --- Blending ---\n",
        "\n",
        "# Split training data further for blending\n",
        "X_train_blend, X_val_blend, y_train_blend, y_val_blend = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train base models on the blend training set\n",
        "blend_models = [\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "]\n",
        "\n",
        "blend_predictions = []\n",
        "for model in blend_models:\n",
        "    model.fit(X_train_blend, y_train_blend)\n",
        "    blend_predictions.append(model.predict_proba(X_val_blend)[:, 1])  # Get probabilities for class 1\n",
        "\n",
        "# Stack blend predictions horizontally\n",
        "blend_predictions = pd.DataFrame(blend_predictions).T\n",
        "\n",
        "# Train meta-model on blend predictions\n",
        "meta_model.fit(blend_predictions, y_val_blend)\n",
        "\n",
        "# Make predictions on the test set using blending\n",
        "test_predictions = []\n",
        "for model in blend_models:\n",
        "    test_predictions.append(model.predict_proba(X_test)[:, 1])\n",
        "test_predictions = pd.DataFrame(test_predictions).T\n",
        "\n",
        "y_pred_blending = meta_model.predict(test_predictions)\n",
        "\n",
        "# Evaluate the blending model\n",
        "accuracy_blending = accuracy_score(y_test, y_pred_blending)\n",
        "print('Blending Accuracy:', accuracy_blending)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScIUcwQg1TUZ",
        "outputId": "c40017d3-d85a-4dae-8ffa-b34f043ff259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.86\n",
            "Blending Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deep neural networks\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize text data (assuming 'Article' column contains text)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Encode labels (assuming binary classification)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the deep neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.2))  # Add dropout for regularization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)  # You can adjust the learning rate\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Make predictions (if needed)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Additional evaluation metrics (if needed)\n",
        "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "# precision = precision_score(y_test, y_pred)\n",
        "# recall = recall_score(y_test, y_pred)\n",
        "# f1 = f1_score(y_test, y_pred)\n",
        "# print(\"Precision:\", precision)\n",
        "# print(\"Recall:\", recall)\n",
        "# print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1wU8M0D287w",
        "outputId": "3eab031b-c60e-4188-cea7-ec1aa47d8511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.7914 - loss: 0.6647 - val_accuracy: 0.7700 - val_loss: 0.5768\n",
            "Epoch 2/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7637 - loss: 0.5247 - val_accuracy: 0.7700 - val_loss: 0.5030\n",
            "Epoch 3/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7654 - loss: 0.4217 - val_accuracy: 0.7700 - val_loss: 0.4565\n",
            "Epoch 4/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7611 - loss: 0.3208 - val_accuracy: 0.7700 - val_loss: 0.3978\n",
            "Epoch 5/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7804 - loss: 0.2350 - val_accuracy: 0.7900 - val_loss: 0.3652\n",
            "Epoch 6/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9770 - loss: 0.1456 - val_accuracy: 0.8100 - val_loss: 0.3555\n",
            "Epoch 7/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1195 - val_accuracy: 0.8700 - val_loss: 0.3276\n",
            "Epoch 8/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0698 - val_accuracy: 0.9100 - val_loss: 0.2795\n",
            "Epoch 9/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0309 - val_accuracy: 0.9200 - val_loss: 0.2622\n",
            "Epoch 10/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 0.9200 - val_loss: 0.2593\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9253 - loss: 0.2438 \n",
            "Accuracy: 0.9200000166893005\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#support vector machines with custom kernals\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Import numpy library\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('labelled_train_set.csv')  # Replace 'your_data.csv' with the actual file name\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Vectorize the 'Article' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X['Article'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Kernel Function (Example: Polynomial Kernel)\n",
        "def polynomial_kernel(X, Y, degree=3, gamma=1, coef0=1):\n",
        "    # Convert the result of the dot product to a dense array before adding coef0\n",
        "    return (gamma * np.dot(X, Y.T).toarray() + coef0) ** degree\n",
        "\n",
        "# Create an SVM model with a custom kernel\n",
        "model = SVC(kernel=polynomial_kernel)  # Use your custom kernel function here\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Generate a classification report for more detailed evaluation\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpnB24f_3kud",
        "outputId": "9ab6b3d8-3639-43d7-9478-ca9718426ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            " AI-generated       1.00      0.48      0.65        23\n",
            "Human-written       0.87      1.00      0.93        77\n",
            "\n",
            "     accuracy                           0.88       100\n",
            "    macro avg       0.93      0.74      0.79       100\n",
            " weighted avg       0.90      0.88      0.86       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# factorization machines\n",
        "!pip install pyfm\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pyfm import FM # Import FM class directly from pyfm\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the data\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "# Separate features and target\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type'].apply(lambda x: 1 if x == 'AI-generated' else 0)  # Convert labels to binary (0 and 1)\n",
        "\n",
        "# Convert categorical features to dictionaries\n",
        "X_dict = X.to_dict(orient='records')\n",
        "\n",
        "# Vectorize features using DictVectorizer\n",
        "v = DictVectorizer()\n",
        "X_vec = v.fit_transform(X_dict)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize FM model\n",
        "fm = FM(num_factors=10, num_iter=100, verbose=True, task=\"classification\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\") # Instantiate FM class\n",
        "\n",
        "# Train the model\n",
        "fm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = fm.predict(X_test)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "gUjqobx15cCy",
        "outputId": "fdeca0ac-97d2-4415-a9d7-deb9ce1a616c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyfm in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyfm) (2.31.0)\n",
            "Requirement already satisfied: urwid>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from pyfm) (2.6.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (4.12.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (0.2.13)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'FM' from 'pyfm' (/usr/local/lib/python3.10/dist-packages/pyfm/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-9bea06ce10ed>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyfm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFM\u001b[0m \u001b[0;31m# Import FM class directly from pyfm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FM' from 'pyfm' (/usr/local/lib/python3.10/dist-packages/pyfm/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "# field-aware factorization machines\n",
        "\n",
        "!pip install pyfm\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pyfm import FM # Import FM class directly from pyfm\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "# Separate features and target\n",
        "X = data.drop(['Type', 'ID'], axis=1)\n",
        "y = data['Type'].apply(lambda x: 1 if x == 'AI-generated' else 0)  # Convert labels to binary (0 and 1)\n",
        "\n",
        "# Convert categorical features to dictionaries\n",
        "X_dict = X.to_dict(orient='records')\n",
        "\n",
        "# Vectorize features using DictVectorizer\n",
        "v = DictVectorizer()\n",
        "X_vec = v.fit_transform(X_dict)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize FM model\n",
        "fm = FM(num_factors=10, num_iter=100, verbose=True, task=\"classification\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\") # Instantiate FM class\n",
        "\n",
        "# Train the model\n",
        "fm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = fm.predict(X_test)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "qTlADiIH5zR2",
        "outputId": "fe69ff80-0d61-4ff2-f2f3-3ec6491a9fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyfm in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyfm) (2.31.0)\n",
            "Requirement already satisfied: urwid>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from pyfm) (2.6.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (4.12.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (0.2.13)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'FM' from 'pyfm' (/usr/local/lib/python3.10/dist-packages/pyfm/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-fafe20484733>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyfm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFM\u001b[0m \u001b[0;31m# Import FM class directly from pyfm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FM' from 'pyfm' (/usr/local/lib/python3.10/dist-packages/pyfm/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# neural collaborative filtering\n",
        "\n",
        "!pip install implicit\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Assuming 'ID' is the user ID, 'Article' is the item ID, and 'Type' is the binary target\n",
        "user_col = 'ID'\n",
        "item_col = 'Article'\n",
        "target_col = 'Type'\n",
        "\n",
        "# Encode user and item IDs\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "data[user_col] = user_encoder.fit_transform(data[user_col])\n",
        "data[item_col] = item_encoder.fit_transform(data[item_col])\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "data[target_col] = target_encoder.fit_transform(data[target_col])\n",
        "\n",
        "# Create a sparse matrix from the interactions\n",
        "rows = data[user_col].values\n",
        "cols = data[item_col].values\n",
        "values = np.ones(len(data))  # Implicit feedback, all interactions are treated equally\n",
        "interaction_matrix = coo_matrix((values, (rows, cols))).tocsr()\n",
        "\n",
        "# Split the data (you might need a different strategy depending on your data)\n",
        "train_users, test_users = train_test_split(data[user_col].unique(), test_size=0.2, random_state=42)\n",
        "train_data = data[data[user_col].isin(train_users)]\n",
        "test_data = data[data[user_col].isin(test_users)]\n",
        "# Here, we're just using a simple random split for demonstration\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]\n",
        "\n",
        "# Create interaction matrices for train and test\n",
        "train_rows = train_data[user_col].values\n",
        "train_cols = train_data[item_col].values\n",
        "train_values = np.ones(len(train_data))\n",
        "train_matrix = coo_matrix((train_values, (train_rows, train_cols))).tocsr()\n",
        "\n",
        "test_rows = test_data[user_col].values\n",
        "test_cols = test_data[item_col].values\n",
        "test_values = np.ones(len(test_data))\n",
        "test_matrix = coo_matrix((test_values, (test_rows, test_cols))).tocsr()\n",
        "\n",
        "# Initialize and train the ALS model\n",
        "model = AlternatingLeastSquares(factors=64, regularization=0.01, iterations=15)\n",
        "alpha = 15  # Confidence scaling factor for implicit feedback\n",
        "model.fit((train_matrix * alpha).astype('double'))\n",
        "\n",
        "# Generate predictions for the test set\n",
        "user_ids = test_data[user_col].unique()\n",
        "item_ids = np.arange(interaction_matrix.shape[1])  # All item IDs\n",
        "predictions = {}\n",
        "for user_id in user_ids:\n",
        "    try:\n",
        "        user_index = user_encoder.transform([user_id])[0]\n",
        "        recommendations = model.recommend(user_index, train_matrix, topn=len(item_ids))\n",
        "        predictions[user_id] = [item_encoder.inverse_transform([item_id])[0] for item_id, _ in recommendations]\n",
        "    except KeyError:\n",
        "        # Handle cases where the user ID was not seen during training\n",
        "        predictions[user_id] = []  # Or any default behavior you prefer\n",
        "\n",
        "# Evaluate the model (adapt this based on your specific needs)\n",
        "y_true = test_data[target_col].values\n",
        "y_pred = [target_encoder.transform([1])[0] if item in predictions.get(user, []) else target_encoder.transform([0])[0]\n",
        "          for user, item in zip(test_data[user_col].values, test_data[item_col].values)]\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "6239a53c211d4a27a29c1ef9214901cd",
            "5ae49daf3cbc475d8176b2bb1687cc4a",
            "358bdf1e4313404fb3c9a4d4b52bcfe6",
            "ad82f25131ed406faf7f2b53df9bd76d",
            "a120bad639ea4abb8f536b4b0a700efb",
            "0c3fe1daef1941238450a7a61422bd97",
            "3e97ae4766e54c9e87d0311d02865fac",
            "da69feb4b1eb4c9a8406191bde8e386c",
            "cfcb5e45709a48458b870d7282d5cbba",
            "f5d0e01d1efb4d92ad60b6c1f80c52e7",
            "87eec307f80d4afd94548ae64e47ee8b"
          ]
        },
        "id": "Sw6clgYx6akw",
        "outputId": "4983c1ab-9a93-48d5-c5b0-61206c815954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: implicit in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from implicit) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from implicit) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from implicit) (4.66.4)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from implicit) (3.5.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6239a53c211d4a27a29c1ef9214901cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y contains previously unseen labels: 336",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 336",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-2225746ceb24>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0muser_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mrecommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 336"
          ]
        }
      ]
    }
  ]
}